# DeepLearning

***神经网络架构介绍***
对各类神经元，神经细胞层，典型神经网络的总结  

***神经元***  
**基本的人工神经网络神经元（basic neural network cell)** 也称前馈神经元    
这种神经元与其它神经元之间的连接具有权重，它可以和前一层神经网络层中的所有神经元有连接。和这个神经元连接的所有神经元的值都会乘以各自对应的权重。  
然后，把这些值都求和。在这个基础上，会额外加上一个**bias 偏置**，它可以用来避免输出为零的情况，并且能够加速某些操作，这让解决某个问题所需要的神经元数量也有所减少。  
这个bias也是一个数字，有些时候是一个常量（经常是-1或者1），有些时候会有所变化。这个总和最终被输入到一个激活函数，这个激活函数的输出最终就成为这个神经元的输出。  

**卷积神经元（Convolutional cells）**  
和前馈神经元非常相似，除了它们只跟前一神经细胞层的部分神经元有连接。因为它们不是和某些神经元随机连接的，而是与特定范围内的神经元相连接，通常用来保存空间信息。这让它们对于那些拥有大量局部信息，比如图像数据、语音数据（但多数情况下是图像数据），会非常实用。  
**对于特定范围的理解：就跟那个拉普拉斯算子或是什么算子，前面特定范围的五个决定后面一个的值，将每个可相同理解为这里的神经元**  

**解卷积神经元**  
与卷积神经元恰好相反：它们是通过跟下一神经细胞层的连接来解码空间信息。这两种神经元都有很多副本，它们都是独立训练的；每个副本都有自己的权重，但连接方式却完全相同。可以认为，这些副本是被放在了具备相同结构的不同的神经网络中。这两种神经元本质上都是一般意义上的神经元，但是，它们的使用方式却不同。  

**池化神经元（Pooling cells）**  
接受到来自其它神经元的输出过后，决定哪些值可以通过，哪些值不能通过。在这个池化的过程当中，图像的大小也会相应地减少，看不到所有的像素了。池化函数会知道什么像素该保留，什么像素该舍弃。  

**插值神经元（interpolating cells）**  
与池化神经元相反，它们获取一些信息，然后映射出更多的信息。额外的信息都是按照某种方式制造出来的，这就好像在一张小分辨率的图片上面进行放大。插值神经元不仅仅是池化神经元的反向操作，而且，它们也是很常见，因为它们运行非常快，同时，实现起来也很简单。池化神经元和插值神经元之间的关系，就像卷积神经元和解卷积神经元之间的关系。  
**池化神经元和插值神经元经常和卷积神经元结合起来使用。**  

**均值神经元和标准方差神经元（Mean and standard deviation cells）**  
（作为概率神经元它们总是成对地出现）是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个**高斯随机数生成器**，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是**全连接**，而且，它们**没有偏差（bias）**。  

**循环神经元（Recurrent cells ）**  
不仅仅在神经细胞层之间有连接，而且在时间轴上也有相应的连接。每一个神经元内部都会保存它先前的值。它们跟一般的神经元一样更新，但是，具有额外的权重：与当前神经元之前值之间的权重，还有大多数情况下，与同一神经细胞层各个神经元之间的权重。当前值和存储的先前值之间权重的工作机制，与非永久性存储器（比如RAM）的工作机制很相似，继承了两个性质：  
**第一，维持一个特定的状态；
第二：如果不对其持续进行更新（输入），这个状态就会消失。**  
由于先前的值是通过激活函数得到的，而在每一次的更新时，都会把这个值和其它权重一起输入到激活函数，因此，信息会不断地流失。实际上，信息的保存率非常的低，以至于仅仅四次或者五次迭代更新过后，几乎之前所有的信息都会流失掉。  

**长短期记忆神经元（Long short term memory cells）**  
用于克服循环神经元中信息快速流失的问题。  
LSTM是一个逻辑回路，其设计受到了计算机内存单元设计的启发。与只存储两个状态的循环神经元相比，LSTM可以存储四个状态：输出值的当前和先前值，记忆神经元状态的当前值和先前值。它们都有三个门：输入门，输出门，遗忘门，同时，它们也还有常规的输入。  
这些门它们都有各自的权重，也就是说，与这种类型的神经元细胞连接需要设置四个权重（而不是一个）。这些门的工作机制与流门（flow gates）很相似，而不是栅栏门（fence gates）：它们可以让所有的信息都通过，或者只是通过部分，也可以什么都不让通过，或者通过某个区间的信息。  
这种运行机制的实现是通过把输入信息和一个在0到1之间的系数相乘，这个系数存储在当前门中。这样，输入门决定输入的信息有多少可以被叠加到当前门值。输出门决定有多少输出信息是可以传递到后面的神经网络中。遗忘门并不是和输出神经元的先前值相连接，而是，和前一记忆神经元相连接。它决定了保留多少记忆神经元最新的状态信息。因为没有和输出相连接，以及没有激活函数在这个循环中，因此只会有更少的信息流失。  

**门控循环神经元（Gated recurrent units (cells)）**  
是LSTM的变体。它们同样使用门来抑制信息的流失，**但是只用两个门：更新门和重置门。**这使得构建它们付出的代价没有那么高，而且运行速度更加快了，因为它们在所有的地方使用了更少的连接。从本质上来说LSTM和GRU有两个不同的地方：  
**第一：GRU神经元没有被输出门保护的隐神经元；
第二：GRU把输出门和遗忘门整合在了一起，形成了更新门。核心的思想就是如果你想要一些新的信息，那么你就可以遗忘掉一些陈旧的信息（反过来也可以）。**  



***神经细胞层***   
**全连接 把所有的神经元与其它所有的神经元相连接**  
这就好像Hopfield神经网络和玻尔兹曼机（Boltzmann machines）的连接方式。当然，这也就意味着连接数量会随着神经元个数的增加呈指数级地增加，但是，对应的函数表达力也会越来越强。  
经历了一段时间的发展，发现把神经网络分解成不同的神经细胞层会非常有效。神经细胞层的定义是一群彼此之间互不连接的神经元，它们仅跟其它神经细胞层有连接。这一概念在受限玻尔兹曼机（Restricted Boltzmann Machines）中有所体现。现在，使用神经网络就意味着使用神经细胞层，并且是任意数量的神经细胞层。其中一个比较令人困惑的概念是全连接（fully connected or completely connected），也就是某一层的每个神经元跟另一层的所有神经元都有连接，但真正的全连接神经网络相当罕见。  
**对这段话的理解：分解成不同的细胞层，每一层之间互不连接，层间联系，已知的大多数网络架构都是这种。**  

**卷积连接层（Convolutionally connected layers）**  
相对于全连接层要有更多的限制：在卷积连接层中的每一个神经元只与相邻的神经元层连接。图像和声音蕴含了大量的信息，如果一对一地输入到神经网络（比如，一个神经元对应一个像素）。卷积连接的形成，受益于保留空间信息更为重要的观察。实践证明这是一个非常好的猜测，因为现在大多数基于人工神经网络的图像和语音应用都使用了这种连接方式。然而，这种连接方式所需的代价远远低于全连接层的形式。从本质上来讲，卷积连接方式起到重要性过滤的作用，决定哪些紧紧联系在一起的信息包是重要的；**卷积连接对于数据降维非常有用。**  

**随机连接神经元（randomly connected neurons）**  
第一，允许部分神经元进行全连接。  
第二，神经元层之间只有部分连接。  

**时间滞后连接（Time delayed connections）**  
是指相连的神经元（通常是在同一个神经元层，甚至于一个神经元自己跟自己连接），它们不从前面的神经元层获取信息，而是从神经元层先前的状态获取信息。这使得暂时（时间上或者序列上）联系在一起的信息可以被存储起来。这些形式的连接经常被手工重新进行设置，从而可以清除神经网络的状态。和常规连接的主要区别是，这种连接会持续不断地改变，即便这个神经网络当前没有处于训练状态。  



***神经网络模型***  
**前馈神经网络（FFNN）**  
单独的神经细胞层内部，神经元之间互不相连；而一般相邻的两个神经细胞层则是全连接（一层的每个神经元和另一层的每一个神经元相连）。一个最简单却最具有实用性的神经网络由两个输入神经元和一个输出神经元构成，也就是一个逻辑门模型。给神经网络一对数据集（分别是“输入数据集”和“我们期望的输出数据集”），一般通过反向传播算法来训练前馈神经网络（FFNNs）。  
**这就是所谓的监督式学习。**与此相反的是无监督学习：我们只给输入，然后让神经网络去寻找数据当中的规律。反向传播的误差往往是神经网络当前输出和给定输出之间差值的某种变体（比如MSE或者仅仅是差值的线性变化）。如果神经网络具有足够的隐层神经元，那么理论上它总是能够建立输入数据和输出数据之间的关系。在实践中，FFNN的使用具有很大的局限性，但是，它们通常和其它神经网络一起组合成新的架构。  
其中以径向基核函数作为激活函数的前馈神经网络被称为**径向神经网络（RBF：Radial basis function）**，其他的大多统称前馈神经网络  

**霍普菲尔网络（HN）   不认为是神经网络**  
是一种每一个神经元都跟其它神经元相互连接的网络。训练前的每一个节点都是输入神经元，训练阶段是隐神经元，输出阶段则是输出神经元。  
该神经网络的训练，是先把神经元的值设置到期望模式，然后计算相应的权重。在这以后，权重将不会再改变了。一旦网络被训练包含一种或者多种模式，这个神经网络总是会收敛于其中的某一种学习到的模式，因为它只会在某一个状态才会稳定。值得注意的是，它并不一定遵从那个期望的状态（很遗憾，它并不是那个具有魔法的黑盒子）。它之所以会稳定下来，部分要归功于在训练期间整个网络的“能量（Energy）”或“温度（Temperature）”会逐渐地减少。每一个神经元的激活函数阈值都会被设置成这个温度的值，一旦神经元输入的总和超过了这个阈值，那么就会让当前神经元选择状态（通常是-1或1，有时也是0或1）。  
可以多个神经元同步，也可以一个神经元一个神经元地对网络进行更新。一旦所有的神经元都已经被更新，并且它们再也没有改变，整个网络就算稳定（退火）了，那你就可以说这个网络已经收敛了。这种类型的网络被称为“联想记忆（associative memory）”，因为它们会收敛到和输入最相似的状态；比如，人类看到桌子的一半就可以想象出另外一半；与之相似，如果输入一半噪音+一半桌子，这个网络就能收敛到整张桌子。  

**玻尔兹曼机（BM）**  


 
