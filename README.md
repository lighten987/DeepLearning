# DeepLearning

***神经网络架构介绍***
对各类神经元，神经细胞层，典型神经网络的总结  

***神经元***  
**基本的人工神经网络神经元（basic neural network cell)** 也称前馈神经元    
这种神经元与其它神经元之间的连接具有权重，它可以和前一层神经网络层中的所有神经元有连接。和这个神经元连接的所有神经元的值都会乘以各自对应的权重。  
然后，把这些值都求和。在这个基础上，会额外加上一个**bias 偏置**，它可以用来避免输出为零的情况，并且能够加速某些操作，这让解决某个问题所需要的神经元数量也有所减少。  
这个bias也是一个数字，有些时候是一个常量（经常是-1或者1），有些时候会有所变化。这个总和最终被输入到一个激活函数，这个激活函数的输出最终就成为这个神经元的输出。  

**卷积神经元（Convolutional cells）**  
和前馈神经元非常相似，除了它们只跟前一神经细胞层的部分神经元有连接。因为它们不是和某些神经元随机连接的，而是与特定范围内的神经元相连接，通常用来保存空间信息。这让它们对于那些拥有大量局部信息，比如图像数据、语音数据（但多数情况下是图像数据），会非常实用。  
**对于特定范围的理解：就跟那个拉普拉斯算子或是什么算子，前面特定范围的五个决定后面一个的值，将每个可相同理解为这里的神经元**  

**解卷积神经元**  
与卷积神经元恰好相反：它们是通过跟下一神经细胞层的连接来解码空间信息。这两种神经元都有很多副本，它们都是独立训练的；每个副本都有自己的权重，但连接方式却完全相同。可以认为，这些副本是被放在了具备相同结构的不同的神经网络中。这两种神经元本质上都是一般意义上的神经元，但是，它们的使用方式却不同。  

**池化神经元（Pooling cells）**  
接受到来自其它神经元的输出过后，决定哪些值可以通过，哪些值不能通过。在这个池化的过程当中，图像的大小也会相应地减少，看不到所有的像素了。池化函数会知道什么像素该保留，什么像素该舍弃。  

**插值神经元（interpolating cells）**  
与池化神经元相反，它们获取一些信息，然后映射出更多的信息。额外的信息都是按照某种方式制造出来的，这就好像在一张小分辨率的图片上面进行放大。插值神经元不仅仅是池化神经元的反向操作，而且，它们也是很常见，因为它们运行非常快，同时，实现起来也很简单。池化神经元和插值神经元之间的关系，就像卷积神经元和解卷积神经元之间的关系。  
**池化神经元和插值神经元经常和卷积神经元结合起来使用。**  

**均值神经元和标准方差神经元（Mean and standard deviation cells）**  
（作为概率神经元它们总是成对地出现）是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个高斯随机数生成器，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是全连接，而且，它们没有偏差（bias）。
