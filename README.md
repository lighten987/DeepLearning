# DeepLearning

***神经网络架构介绍***
对各类神经元，神经细胞层，典型神经网络的总结  

***神经元***  
**基本的人工神经网络神经元（basic neural network cell)** 也称前馈神经元    
这种神经元与其它神经元之间的连接具有权重，它可以和前一层神经网络层中的所有神经元有连接。和这个神经元连接的所有神经元的值都会乘以各自对应的权重。  
然后，把这些值都求和。在这个基础上，会额外加上一个**bias 偏置**，它可以用来避免输出为零的情况，并且能够加速某些操作，这让解决某个问题所需要的神经元数量也有所减少。  
这个bias也是一个数字，有些时候是一个常量（经常是-1或者1），有些时候会有所变化。这个总和最终被输入到一个激活函数，这个激活函数的输出最终就成为这个神经元的输出。  

**卷积神经元（Convolutional cells）**  
和前馈神经元非常相似，除了它们只跟前一神经细胞层的部分神经元有连接。因为它们不是和某些神经元随机连接的，而是与特定范围内的神经元相连接，通常用来保存空间信息。这让它们对于那些拥有大量局部信息，比如图像数据、语音数据（但多数情况下是图像数据），会非常实用。  
**对于特定范围的理解：就跟那个拉普拉斯算子或是什么算子，前面特定范围的五个决定后面一个的值，将每个可相同理解为这里的神经元**  

**解卷积神经元**  
与卷积神经元恰好相反：它们是通过跟下一神经细胞层的连接来解码空间信息。这两种神经元都有很多副本，它们都是独立训练的；每个副本都有自己的权重，但连接方式却完全相同。可以认为，这些副本是被放在了具备相同结构的不同的神经网络中。这两种神经元本质上都是一般意义上的神经元，但是，它们的使用方式却不同。  

**池化神经元（Pooling cells）**  
接受到来自其它神经元的输出过后，决定哪些值可以通过，哪些值不能通过。在这个池化的过程当中，图像的大小也会相应地减少，看不到所有的像素了。池化函数会知道什么像素该保留，什么像素该舍弃。  

**插值神经元（interpolating cells）**  
与池化神经元相反，它们获取一些信息，然后映射出更多的信息。额外的信息都是按照某种方式制造出来的，这就好像在一张小分辨率的图片上面进行放大。插值神经元不仅仅是池化神经元的反向操作，而且，它们也是很常见，因为它们运行非常快，同时，实现起来也很简单。池化神经元和插值神经元之间的关系，就像卷积神经元和解卷积神经元之间的关系。  
**池化神经元和插值神经元经常和卷积神经元结合起来使用。**  

**均值神经元和标准方差神经元（Mean and standard deviation cells）**  
（作为概率神经元它们总是成对地出现）是一类用来描述数据概率分布的神经元。均值就是所有值的平均值，而标准方差描述的是这些数据偏离（两个方向）均值有多远。比如：一个用于图像处理的概率神经元可以包含一些信息，比如：在某个特定的像素里面有多少红色。举个例来说，均值可能是0.5，同时标准方差是0.2。当要从这些概率神经元取样的时候，你可以把这些值输入到一个**高斯随机数生成器**，这样就会生成一些分布在0.4和0.6之间的值；值离0.5越远，对应生成的概率也就越小。它们一般和前一神经元层或者下一神经元层是**全连接**，而且，它们**没有偏差（bias）**。  

**循环神经元（Recurrent cells ）**  
不仅仅在神经细胞层之间有连接，而且在时间轴上也有相应的连接。每一个神经元内部都会保存它先前的值。它们跟一般的神经元一样更新，但是，具有额外的权重：与当前神经元之前值之间的权重，还有大多数情况下，与同一神经细胞层各个神经元之间的权重。当前值和存储的先前值之间权重的工作机制，与非永久性存储器（比如RAM）的工作机制很相似，继承了两个性质：  
**第一，维持一个特定的状态；
第二：如果不对其持续进行更新（输入），这个状态就会消失。**  
由于先前的值是通过激活函数得到的，而在每一次的更新时，都会把这个值和其它权重一起输入到激活函数，因此，信息会不断地流失。实际上，信息的保存率非常的低，以至于仅仅四次或者五次迭代更新过后，几乎之前所有的信息都会流失掉。  

**长短期记忆神经元（Long short term memory cells）**用于克服循环神经元中信息快速流失的问题。  
LSTM是一个逻辑回路，其设计受到了计算机内存单元设计的启发。与只存储两个状态的循环神经元相比，LSTM可以存储四个状态：输出值的当前和先前值，记忆神经元状态的当前值和先前值。它们都有三个门：输入门，输出门，遗忘门，同时，它们也还有常规的输入。  
这些门它们都有各自的权重，也就是说，与这种类型的神经元细胞连接需要设置四个权重（而不是一个）。这些门的工作机制与流门（flow gates）很相似，而不是栅栏门（fence gates）：它们可以让所有的信息都通过，或者只是通过部分，也可以什么都不让通过，或者通过某个区间的信息。  
这种运行机制的实现是通过把输入信息和一个在0到1之间的系数相乘，这个系数存储在当前门中。这样，输入门决定输入的信息有多少可以被叠加到当前门值。输出门决定有多少输出信息是可以传递到后面的神经网络中。遗忘门并不是和输出神经元的先前值相连接，而是，和前一记忆神经元相连接。它决定了保留多少记忆神经元最新的状态信息。因为没有和输出相连接，以及没有激活函数在这个循环中，因此只会有更少的信息流失。  

**门控循环神经元（Gated recurrent units (cells)）**  
是LSTM的变体。它们同样使用门来抑制信息的流失，**但是只用两个门：更新门和重置门。**这使得构建它们付出的代价没有那么高，而且运行速度更加快了，因为它们在所有的地方使用了更少的连接。从本质上来说LSTM和GRU有两个不同的地方：  
**第一：GRU神经元没有被输出门保护的隐神经元；
第二：GRU把输出门和遗忘门整合在了一起，形成了更新门。核心的思想就是如果你想要一些新的信息，那么你就可以遗忘掉一些陈旧的信息（反过来也可以）。**  


**神经细胞层**  



 
